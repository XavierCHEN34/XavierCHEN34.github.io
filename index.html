<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xi CHEN (陈汐)</title>

  <meta name="author" content="Xi Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/hku_icon.jpeg">

  <script type="text/javascript">

    function display(id) {
      var traget = document.getElementById(id);
      if (traget.style.display == "none") {
        traget.style.display = "";
      } else {
        traget.style.display = "none";
      }
    }  
  </script>
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Xi CHEN<img style="max-height:30px;vertical-align: middle;"
                        src="images/chinese_name.png" /></name>

                  </p>
                  <p>
                    I am a third-year (starting from 2022) Ph.D. student at <a href="https://www.hku.hk/">the University of Hong Kong</a>, supervised by Prof. <a
                    href="https://hszhao.github.io/">Hengshuang Zhao</a>.

                    Before joining HKU, I worked as a senior algorithm engineer at  
                    <a href="https://www.alibabagroup.com/en-US">Alibaba Group</a>.
          
                    Previously, I got the Master degree at 
                    <a href="https://www.zju.edu.cn/english/">Zhejiang University</a> in 2020. 
                    <!--supervised by Prof.  <a
                    href="https://person.zju.edu.cn/qidonglian/">Donglian Qi</a> and Prof.  <a
                    href="https://mypage.zju.edu.cn/jxs/">Jianxin Shen</a>.  -->
                    I also got a double master diploma at 
                    <a href="https://www.centrale-mediterranee.fr/fr">Ecole Centrale Mediterranee(France)</a>.
                    Before that, I received my B.Eng. from Zhejiang University in 2017. 
                  </p>


                  <p>
                    My research interests lie in the field of deep learning and computer vision, I've published multiple research works for image/video perception, open-world multi-modal learning.
                    Now I have interests in AIGC, MLLM, and reinforcement learning. 
                  </p>

                  <p>
                    I've done internship or cooperations with many companies like 
                     <a href="https://www.adobe.com/about-adobe.html">Adobe</a>, 
                     <a href=" https://www.meta.ai/">Meta</a>, 
                     <a href="https://en.megvii.com/">Megvii</a>, 
                     <a href="https://www.sensetime.com/cn">SenseTime</a>, 
                     <a href="https://us.hikvision.com/en">Hikvision</a>, 
                     <a href="https://www.ufotosoft.com/">Ufoto</a>, 
                     <a href="https://www.aisegment.cn/">AISegment</a>, 
                     <a href="https://www.lis-lab.fr/">LIS(France)</a>, 
                     etc.
                  </p>

                  <p>
                    <span style="color: red;">I co-supervise interns with some industrial groups.</span>  
                    Contact me over gmail (xichen.csai AT gmail) if you are looking for internships or you would like to cooperate with me.
                  </p>
                
                  <p>
                    <!-- <font color="red">Update: On job market now! Drop me an email if you are interested.</font> -->
                  </p>
                  <p style="text-align:center">
                    <a href="data/cv_chenxi_2023.3.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=INISnXkAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/XavierCHEN34">Github</a> &nbsp/&nbsp
                    <a href="https://www.zhihu.com/people/xavierchen">Zhihu</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/photo-life.jpg"><img style="width:80%;max-width:80" alt="profile photo"
                      src="images/photo-life.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <ul>
                    <li> <b>[Apr. 2025]</b> Four papers accepted by Siggraph2025!</li>
                    <li> <b>[Mar. 2025]</b> Four papers accepted by CVPR2025 (two Highlights)!</li>
                    <li> <b>[Oct. 2024]</b> Two papers accepted by NeurIPS2024!</li>
                    <li> <b>[July. 2024]</b> Four papers accepted by ECCV2024, see you at Milano!</li>
                    <li> <b>[Jun. 2024]</b> We release <a href="https://xavierchen34.github.io/MimicBrush-Page/">MimicBrush</a> for imitative image editing. </li>
                    <li> <b>[Dec. 2023]</b> We release <a href="https://xavierchen34.github.io/LivePhoto-Page/">LivePhoto</a> for image-to-video generation. </li>
                    <li> <b>[Dec. 2023]</b> The code of AnyDoor is released <a href="https://github.com/damo-vilab/AnyDoor">here</a>, we would continue to make it stronger. </li>
                    <li> <b>[Jul. 2023]</b> We release <a href="https://damo-vilab.github.io/AnyDoor-Page/">AnyDoor</a> for zero-shot image composition & customization.</li>
                    <li> <b>[Jul. 2023]</b> OPSNet is accepted by ICCV2023.</li>
                    <li> <b>[Mar. 2023]</b> We release the manuscripts of 
                      <a href="https://arxiv.org/abs/2303.11324">OPSNet</a> and 
                      <a href="https://arxiv.org/abs/2303.11320">ScribbleSeg</a>.</li>
                    <li> <b>[Mar. 2023]</b> One co-authored paper accepted to CVPR2023.</li>
                    <li> <b>[Feb. 2022]</b> I join HKU as a PhD student. </li>
                    <a onclick="return display('old_news');"> ---- show more ----</a>
                    <div id="old_news" style="display: none;">
                      <li> <b>[Mar. 2022]</b> One paper accepted to CVPR2022</li>
                      <li> <b>[Jun. 2021]</b> One paper accepted to ICCV2021</li>
                      <li> <b>[Jun. 2020]</b> I graduate from Zhejiang University and join Alibaba Group</li>
                      <li> <b>[Mar. 2020]</b> One paper accepted to CVPR2020</li>

                    </div>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Publications
                  <p>
                    <a href="https://scholar.google.com/citations?user=INISnXkAAAAJ&hl=en">Google Scholar</a> 
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/mico.png' width="190">
                    </div>
                    <img src='images/mico2.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2506.22434">
                    <papertitle>MiCo: Multi-image Contrast for Reinforcement Visual Reasoning</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Mingkang Zhu, Shaoteng Liu, Xiaoyang Wu, Xiaogang Xu, Yu Liu, Xiang Bai, Hengshuang Zhao
                  <br>
                  <em>Preprint</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2506.22434">pdf</a> / 
                  <a href="https://x.com/_akhaliq/status/1939528983135363086">media(AK)</a> /
                  <a href="https://arxiv.org/abs/2506.22434">code(todo)</a> 
                  <p></p>
                  <p>
                    MiCo is a self-supervised reinforcement learning framework for multi-image visual reasoning. 
                    It leverages supervision by comparing an image, its augmented view, and a similar image to encourage chain-of-thought (CoT) reasoning through a method we call "Augmented-GRPO."
                  </p>
                </td>
              </tr>



              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/UniReal.gif' width="190">
                    </div>
                    <img src='images/UniReal.gif' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2412.07774">
                    <papertitle>UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, Hui Ding, Zhe Lin, Hengshuang Zhao
                  <br>
                  <em>CVPR</em>, 2025 (Highlight)
                  <br>
                  <a href="https://arxiv.org/abs/2412.07774">pdf</a>/
                  <a href="https://xavierchen34.github.io/UniReal-Page/">page</a>/
                  <a href="https://github.com/XavierCHEN34/UniReal">code (data construction)</a>
                  <p></p>
                  <p>
                    <span style="color: red;">Foundational multi-modal generative model cooperated with Adobe.</span>  
                    UniReal is a universal framework for multiple image generation and editing tasks. We leverage a video model to handld image tasks by treating different numbers of 
                    input/output images as frames. We also seek universal supervisions from video data, thus generating realistic results that understand the world dynamics.
                  </p>
                </td>
              </tr>


              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/MimicBrush.png' width="190">
                    </div>
                    <img src='images/MimicBrush.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2406.07547">
                    <papertitle>Zero-shot Image Editing with Reference Imitation</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, Hengshuang Zhao
                  <br>
                  <em>NeurIPS</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2406.07547">pdf</a>/
                  <a href="https://xavierchen34.github.io/MimicBrush-Page/">page</a>/
                  <a href="https://github.com/ali-vilab/MimicBrush">code</a>/
                  <a href="https://x.com/_akhaliq/status/1800726257098760584">media (AK)</a>/
                  <a href="https://x.com/Gradio/status/1800905251563733361">media (Gradio)</a>
                  <p></p>
                  <p>
                    <span style="color: red;">GithHub 1.1k stars.</span>  
                    MimicBrush conducts imitative editing by discovering the semantic correspondence between the source and reference image.
                    It supports interesting and practical applications for local region composition and texture transfer.
                  </p>
                </td>
              </tr>



  

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/LivePhoto.gif' width="190">
                    </div>
                    <img src='images/LivePhoto.gif' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2312.02928">
                    <papertitle>LivePhoto: Real Image Animation with Text-guided Motion Control</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, Hengshuang Zhao 
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2312.02928">pdf</a>/
                  <a href="https://xavierchen34.github.io/LivePhoto-Page/">page</a>/
                  <a href="https://github.com/XavierCHEN34/LivePhoto">code</a>/
                  <a href="https://twitter.com/_akhaliq/status/1732224399414067333">media (AK)</a>
                  <p></p>
                  <p>
                    We present LivePhoto, a real image animation method with text control. Different from previous works, 
                    LivePhoto truely listens to the text instructions and well preserves the object-ID.
                  </p>
                </td>
              </tr>


              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/AnyDoor_teaser.jpg' width="190">
                    </div>
                    <img src='images/AnyDoor_teaser.jpg' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/pdf/2307.09481.pdf">
                    <papertitle>AnyDoor: Zero-shot Object-level Image Customization</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, Hengshuang Zhao 
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2307.09481.pdf">pdf</a>/
                  <a href="https://damo-vilab.github.io/AnyDoor-Page/">page</a>/
                  <a href="https://github.com/damo-vilab/AnyDoor">code</a>/
                  <a href="https://x.com/_akhaliq/status/1738772616142303728">media[AK]</a>/
                  <a href="https://mp.weixin.qq.com/s/IlRAhuJ4k0sFRJulUKZICA">[量子位]</a>/
                  <a href="https://www.jiqizhixin.com/articles/2023-12-21-3">[机器之心]</a>
                  <p></p>
                  <p>
                    <span style="color: red;">Selected as the most influencial papers of CVPR 2024. GithHub 4.2k stars.</span>  
                    This work presents AnyDoor, a diffusion-based image generator with the power to 
                    teleport target objects to new scenes at user-specified locations in a harmonious way. 
                  </p>
                </td>
              </tr>

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/OPSNet.gif' width="190">
                    </div>
                    <img src='images/OPSNet.gif' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2303.11324">
                    <papertitle>Open-vocabulary Panoptic Segmentation with Embedding Modulation</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Shuang Li, Ser-Nam Lim, Antonio Torralba, Hengshuang Zhao
                  <br>
                  <em>ICCV</em>, 2023

                  <br>
                  <a href="https://arxiv.org/abs/2303.11324">pdf</a>/
                  <a href="https://opsnet-page.github.io/">page</a>
                  <p></p>
                  <p>
                    We present a omnipotent and efficient framework for open-vocabulary panoptic segmentation, which shows great performance
                    for both closed- and open-vocabulary settings with limited training data.
                  </p>
                </td>
              </tr>
              
              
              
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/FocalClick.png' width="190">
                    </div>
                    <img src='images/FocalClick.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2204.02574">
                    <papertitle>FocalClick: Towards Practical Interactive Image Segmentation</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, Hengshuang Zhao
                  <br>
                  <!--<em>ICML</em>, 2022 &nbsp <font color="red"><strong>(Spotlight)</strong></font> -->
                  <em>CVPR</em>, 2022

                  <br>
                  <a href="https://arxiv.org/abs/2204.02574">pdf</a> /
                  <a href="https://github.com/XavierCHEN34/ClickSEG">code</a>
                  <p></p>
                  <p>
                    FocalClick is a simple and effective solution for interactive segmentation. It largely reduces the 
                    computation for various models by focusing on target local regions.
                  </p>
                </td>
              </tr>    


              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/CDNet.png' width="190">
                    </div>
                    <img src='images/CDNet.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Conditional_Diffusion_for_Interactive_Segmentation_ICCV_2021_paper.pdf">
                    <papertitle>Conditional Diffusion for Interactive Segmentation</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, Manni Duan
                  <br>
                  <!--<em>ICML</em>, 2022 &nbsp <font color="red"><strong>(Spotlight)</strong></font> -->
                  <em>ICCV</em>, 2021

                  <br>
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Conditional_Diffusion_for_Interactive_Segmentation_ICCV_2021_paper.pdf">pdf</a> /
                  <a href="https://github.com/XavierCHEN34/ClickSEG">code</a>
                  <p></p>
                  <p>
                    We view interactive segmentation as a diffusion procedure and design  feature- and pixel-level diffuion modules for more consistent predictions.
                  </p>
                </td>
              </tr>    

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/sat.gif' width="190">
                    </div>
                    <img src='images/sat.gif' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2003.00482">
                    <papertitle>State-Aware Tracker for Real-Time Video Object Segmentation</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Zuoxin Li, Ye Yuan, Gang Yu, Jianxin Shen, Donglian Qi
                  <br>
                  <!--<em>ICML</em>, 2022 &nbsp <font color="red"><strong>(Spotlight)</strong></font> -->
                  <em>CVPR</em>, 2020

                  <br>
                  <a href="https://arxiv.org/abs/2003.00482">pdf</a> /
                  <a href="https://github.com/megvii-research/video_analyst">code</a>
                  <p></p>
                  <p>
                    We propose a novel pipeline called State-Aware Tracker (SAT), which can produce accurate segmentation results with real-time speed.
                  </p>
                </td>
              </tr>   

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Projects & Resources</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/SiamFCOT.png' width="190">
                    </div>
                    <img src='images/SiamFCOT.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a>
                    <papertitle>Siamese Fully Convolutional Object Tracking</papertitle>
                  </a>
                  <br>
                  Weizhao Wang, Xinyu Chen,  <strong>Xi Chen</strong>, Yinda Xu, Zeyu Wang
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9022051">pdf</a> /
                  <a href="http://data.votchallenge.net/vot2019/trackers/SiamFCOT-code-2019-06-09T16_22_25.118122.zip">code</a>
                  <p></p>
                  <p>
                    Second place solution for VOT2019 real-time track. SiamFCOT serves as a strong pipeline for real-time 
                    single object tracking.
                  </p>
                </td>
              </tr>


            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">

                  <heading>Academic Service</heading>
                  <p>
                    <b>Reviewer / Program Committee Member</b>
                  </p>
                  <ul>
                    <li> CVPR (2021, 2022, 2023, 2024, 2025)</li>
                    <li> ICCV (2021, 2023)</li>
                    <li> ECCV (2022,2024)</li>
                    <li> Siggraph / Siggraph Asia (2024, 2025)</li>
                    <li> NeurIPS (2023,2024)</li>
                    <li> ICLR (2023)</li>
                    <li> AAAI (2022, 2023)</li>
                    <li> ACM MM (2024)</li>
                    <li> Journals: IJCV, PR, TCSVT, ect</li>    
                  </ul>

                  <p>
                    <b>Orgnizer / Program Chair</b>
                  </p>
                  <ul>
                    <li><a href="https://higen-2025.github.io/" target="_blank" rel="noopener noreferrer">HiGen (ICCV 2025 Workshop)</a></li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=200&t=tt&d=64wFOrCgVspDjUUVP2vRXujCrmintJYyeSI4nWJgn-c'></script>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's
                    website</a>
                </p>
              </td>
            </tr>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>