<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xi CHEN (陈汐)</title>

  <meta name="author" content="Xi Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/hku_icon.jpeg">

  <script type="text/javascript">

    function display(id) {
      var traget = document.getElementById(id);
      if (traget.style.display == "none") {
        traget.style.display = "";
      } else {
        traget.style.display = "none";
      }
    }  
  </script>
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Xi CHEN<img style="max-height:30px;vertical-align: middle;"
                        src="images/chinese_name.png" /></name>

                  </p>
                  <p>
                    I am a Ph.D. student at <a href="https://www.hku.hk/">the University of Hong Kong</a>, supervised by Prof. <a
                    href="https://hszhao.github.io/">Hengshuang Zhao</a>.

                    <!--Before joining HKU, I worked as a senior research engineer at  
                    <a href="https://www.alibabagroup.com/en-US">Alibaba Group</a>.-->
          
                    Previously, I got the Master and Bachelor degree from
                    <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>,
                    I also get a double master degree from
                    <a href="https://www.centrale-mediterranee.fr/fr">Ecole Centrale Mediterranee</a>.
                  </p>


                  <p>
                    I've done internship or cooperations with  
                     <a href="https://deepmind.google/">Google DeepMind</a>, 
                     <a href="https://www.adobe.com/about-adobe.html">Adobe</a>, 
                     <a href="https://www.meta.ai/">Meta</a>, 
                     <a href="https://www.alibaba.com/">Alibaba Tongyi Lab</a>, 
                     <a href="https://en.megvii.com/">Megvii</a>, 
                     <a href="https://www.sensetime.com/cn">SenseTime</a>, 
                     <a href="https://us.hikvision.com/en">Hikvision</a>, 
                     <a href="https://www.ufotosoft.com/">Ufoto</a>, 
                     <a href="https://www.aisegment.cn/">AISegment</a>, 
                     <a href="https://www.lis-lab.fr/">LIS(France)</a>, 
                     etc. Thanks those companies!
                  </p>

                  <p>
                    Now I have interests in AIGC, MLLM, and reinforcement learning. 
                    <span style="color:rgb(175, 9, 9); font-weight:bold;">
                      I co-supervise interns with some industrial groups.
                    </span>  
                    Contact me over gmail (xichen.csai AT gmail) if you are looking for internships or you would like to cooperate with me.
                  </p>
                
                  <p>
                    <!-- <font color="red">Update: On job market now! Drop me an email if you are interested.</font> -->
                  </p>
                  <p style="text-align:center">
                    <a href="data/cv_chenxi_2023.3.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=INISnXkAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/XavierCHEN34">Github</a> &nbsp/&nbsp
                    <a href="https://www.zhihu.com/people/xavierchen">Zhihu</a>
                  </p>
                </td>
              
                <td style="padding:2.5%;width:40%;max-width:40%;text-align:center;">
                  <a href="images/photo-life.jpg" target="_blank">
                    <img 
                      src="images/photo-life.jpg" 
                      alt="profile photo" 
                      class="hoverZoomLink profile-photo"
                      style="
                        width: 220px;
                        height: 220px;
                        border-radius: 50%;
                        object-fit: cover;
                        box-shadow: 0 4px 10px rgba(0,0,0,0.2);
                        transition: all 0.5s ease-in-out;
                      ">
                  </a>
                </td>

                <style>
                  /* 鼠标悬停时换图 */
                  .profile-photo:hover {
                    content: url('images/photo-life2.jpg');
                    transform: scale(1.05); /* 轻微放大一点，更生动 */
                    box-shadow: 0 6px 15px rgba(0,0,0,0.3);
                  }
                </style>


              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <ul>
                    <li> <b>[Sep. 2025]</b> Six papers are accepted by NeurIPS2025! (1 Oral) </li>
                    <li> <b>[Apr. 2025]</b> Four papers accepted by Siggraph2025!</li>
                    <li> <b>[Mar. 2025]</b> Four papers accepted by CVPR2025 (two Highlights)!</li>
                    <li> <b>[Oct. 2024]</b> Two papers accepted by NeurIPS2024!</li>
                    <li> <b>[July. 2024]</b> Four papers accepted by ECCV2024, see you at Milano!</li>
                    <li> <b>[Jun. 2024]</b> We release <a href="https://xavierchen34.github.io/MimicBrush-Page/">MimicBrush</a> for imitative image editing. </li>
                    <li> <b>[Dec. 2023]</b> We release <a href="https://xavierchen34.github.io/LivePhoto-Page/">LivePhoto</a> for image-to-video generation. </li>
                    <li> <b>[Dec. 2023]</b> The code of AnyDoor is released <a href="https://github.com/damo-vilab/AnyDoor">here</a>, we would continue to make it stronger. </li>
                    <li> <b>[Jul. 2023]</b> We release <a href="https://damo-vilab.github.io/AnyDoor-Page/">AnyDoor</a> for zero-shot image composition & customization.</li>
                    <li> <b>[Jul. 2023]</b> OPSNet is accepted by ICCV2023.</li>
                    <li> <b>[Mar. 2023]</b> We release the manuscripts of 
                      <a href="https://arxiv.org/abs/2303.11324">OPSNet</a> and 
                      <a href="https://arxiv.org/abs/2303.11320">ScribbleSeg</a>.</li>
                    <li> <b>[Mar. 2023]</b> One co-authored paper accepted to CVPR2023.</li>
                    <li> <b>[Feb. 2022]</b> I join HKU as a PhD student. </li>
                    <a onclick="return display('old_news');"> ---- show more ----</a>
                    <div id="old_news" style="display: none;">
                      <li> <b>[Mar. 2022]</b> One paper accepted to CVPR2022</li>
                      <li> <b>[Jun. 2021]</b> One paper accepted to ICCV2021</li>
                      <li> <b>[Jun. 2020]</b> I graduate from Zhejiang University and join Alibaba Group</li>
                      <li> <b>[Mar. 2020]</b> One paper accepted to CVPR2020</li>

                    </div>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>First Author Publications
                  <p>
                    <a href="https://scholar.google.com/citations?user=INISnXkAAAAJ&hl=en">Google Scholar</a> 
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/mico.png' width="190">
                    </div>
                    <img src='images/mico2.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2506.22434">
                    <papertitle>MiCo: Multi-image Contrast for Reinforcement Visual Reasoning</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Mingkang Zhu, Shaoteng Liu, Xiaoyang Wu, Xiaogang Xu, Yu Liu, Xiang Bai, Hengshuang Zhao
                  <br>
                  <em>NeurIPS</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2506.22434">pdf</a> / 
                  <a href="https://x.com/_akhaliq/status/1939528983135363086">media(AK)</a> /
                  <a href="https://arxiv.org/abs/2506.22434">code(todo)</a> 
                  <p></p>
                  <p>
                    MiCo is a self-supervised reinforcement learning framework for multi-image visual reasoning. 
                    It leverages supervision by comparing an image, its augmented view, and a similar image to encourage chain-of-thought (CoT) reasoning through a method we call "Augmented-GRPO."
                  </p>
                </td>
              </tr>



              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/UniReal.gif' width="190">
                    </div>
                    <img src='images/UniReal.gif' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2412.07774">
                    <papertitle>UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, Hui Ding, Zhe Lin, Hengshuang Zhao
                  <br>
                  <em>CVPR</em>, 2025 (Highlight)
                  <br>
                  <a href="https://arxiv.org/abs/2412.07774">pdf</a>/
                  <a href="https://xavierchen34.github.io/UniReal-Page/">page</a>/
                  <a href="https://github.com/XavierCHEN34/UniReal">code (data construction)</a>
                  <p></p>
                  <p>
                    <span style="color: rgb(175, 9, 9);">Foundational multi-modal generative model cooperated with Adobe.</span>  
                    UniReal is a universal framework for multiple image generation and editing tasks. We leverage a video model to handld image tasks by treating different numbers of 
                    input/output images as frames. We also seek universal supervisions from video data, thus generating realistic results that understand the world dynamics.
                  </p>
                </td>
              </tr>


              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/MimicBrush.png' width="190">
                    </div>
                    <img src='images/MimicBrush.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2406.07547">
                    <papertitle>Zero-shot Image Editing with Reference Imitation</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, Hengshuang Zhao
                  <br>
                  <em>NeurIPS</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2406.07547">pdf</a>/
                  <a href="https://xavierchen34.github.io/MimicBrush-Page/">page</a>/
                  <a href="https://github.com/ali-vilab/MimicBrush">code</a>/
                  <a href="https://x.com/_akhaliq/status/1800726257098760584">media (AK)</a>/
                  <a href="https://x.com/Gradio/status/1800905251563733361">media (Gradio)</a>
                  <p></p>
                  <p>
                    <span style="color: rgb(175, 9, 9);">GithHub 1.1k stars.</span>  
                    MimicBrush conducts imitative editing by discovering the semantic correspondence between the source and reference image.
                    It supports interesting and practical applications for local region composition and texture transfer.
                  </p>
                </td>
              </tr>



  

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/LivePhoto.gif' width="190">
                    </div>
                    <img src='images/LivePhoto.gif' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2312.02928">
                    <papertitle>LivePhoto: Real Image Animation with Text-guided Motion Control</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, Hengshuang Zhao 
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2312.02928">pdf</a>/
                  <a href="https://xavierchen34.github.io/LivePhoto-Page/">page</a>/
                  <a href="https://github.com/XavierCHEN34/LivePhoto">code</a>/
                  <a href="https://twitter.com/_akhaliq/status/1732224399414067333">media (AK)</a>
                  <p></p>
                  <p>
                    We present LivePhoto, a real image animation method with text control. Different from previous works, 
                    LivePhoto truely listens to the text instructions and well preserves the object-ID.
                  </p>
                </td>
              </tr>


              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/AnyDoor_teaser.jpg' width="190">
                    </div>
                    <img src='images/AnyDoor_teaser.jpg' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }
                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/pdf/2307.09481.pdf">
                    <papertitle>AnyDoor: Zero-shot Object-level Image Customization</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, Hengshuang Zhao 
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2307.09481.pdf">pdf</a>/
                  <a href="https://damo-vilab.github.io/AnyDoor-Page/">page</a>/
                  <a href="https://github.com/damo-vilab/AnyDoor">code</a>/
                  <a href="https://x.com/_akhaliq/status/1738772616142303728">media[AK]</a>/
                  <a href="https://mp.weixin.qq.com/s/IlRAhuJ4k0sFRJulUKZICA">[量子位]</a>/
                  <a href="https://www.jiqizhixin.com/articles/2023-12-21-3">[机器之心]</a>
                  <p></p>
                  <p>
                    <span style="color: rgb(175, 9, 9);">Selected as the most influencial papers of CVPR 2024. GithHub 4.2k stars.</span>  
                    This work presents AnyDoor, a diffusion-based image generator with the power to 
                    teleport target objects to new scenes at user-specified locations in a harmonious way. 
                  </p>
                </td>
              </tr>

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/OPSNet.gif' width="190">
                    </div>
                    <img src='images/OPSNet.gif' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2303.11324">
                    <papertitle>Open-vocabulary Panoptic Segmentation with Embedding Modulation</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Shuang Li, Ser-Nam Lim, Antonio Torralba, Hengshuang Zhao
                  <br>
                  <em>ICCV</em>, 2023

                  <br>
                  <a href="https://arxiv.org/abs/2303.11324">pdf</a>/
                  <a href="https://opsnet-page.github.io/">page</a>
                  <p></p>
                  <p>
                    We present a omnipotent and efficient framework for open-vocabulary panoptic segmentation, which shows great performance
                    for both closed- and open-vocabulary settings with limited training data.
                  </p>
                </td>
              </tr>
              
              
              
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/FocalClick.png' width="190">
                    </div>
                    <img src='images/FocalClick.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2204.02574">
                    <papertitle>FocalClick: Towards Practical Interactive Image Segmentation</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, Hengshuang Zhao
                  <br>
                  <!--<em>ICML</em>, 2022 &nbsp <font color="red"><strong>(Spotlight)</strong></font> -->
                  <em>CVPR</em>, 2022

                  <br>
                  <a href="https://arxiv.org/abs/2204.02574">pdf</a> /
                  <a href="https://github.com/XavierCHEN34/ClickSEG">code</a>
                  <p></p>
                  <p>
                    FocalClick is a simple and effective solution for interactive segmentation. It largely reduces the 
                    computation for various models by focusing on target local regions.
                  </p>
                </td>
              </tr>    


              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/CDNet.png' width="190">
                    </div>
                    <img src='images/CDNet.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Conditional_Diffusion_for_Interactive_Segmentation_ICCV_2021_paper.pdf">
                    <papertitle>Conditional Diffusion for Interactive Segmentation</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, Manni Duan
                  <br>
                  <!--<em>ICML</em>, 2022 &nbsp <font color="red"><strong>(Spotlight)</strong></font> -->
                  <em>ICCV</em>, 2021

                  <br>
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Conditional_Diffusion_for_Interactive_Segmentation_ICCV_2021_paper.pdf">pdf</a> /
                  <a href="https://github.com/XavierCHEN34/ClickSEG">code</a>
                  <p></p>
                  <p>
                    We view interactive segmentation as a diffusion procedure and design  feature- and pixel-level diffuion modules for more consistent predictions.
                  </p>
                </td>
              </tr>    

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/sat.gif' width="190">
                    </div>
                    <img src='images/sat.gif' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2003.00482">
                    <papertitle>State-Aware Tracker for Real-Time Video Object Segmentation</papertitle>
                  </a>
                  <br>
                  <strong>Xi Chen</strong>, Zuoxin Li, Ye Yuan, Gang Yu, Jianxin Shen, Donglian Qi
                  <br>
                  <!--<em>ICML</em>, 2022 &nbsp <font color="red"><strong>(Spotlight)</strong></font> -->
                  <em>CVPR</em>, 2020

                  <br>
                  <a href="https://arxiv.org/abs/2003.00482">pdf</a> /
                  <a href="https://github.com/megvii-research/video_analyst">code</a>
                  <p></p>
                  <p>
                    We propose a novel pipeline called State-Aware Tracker (SAT), which can produce accurate segmentation results with real-time speed.
                  </p>
                </td>
              </tr>   

            </tbody>
          </table>



          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading> Projects I Led </heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/picabench.png' width="190">
                    </div>
                    <img src='images/picabench.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2510.17681">
                    <papertitle>PICABench: How Far Are We from Physically Realistic Image Editing?</papertitle>
                  </a>
                  <br>
                    Yuandong Pu, Le Zhuo, Songhao Han, Jinbo Xing, Kaiwen Zhu, Shuo Cao, Bin Fu, Si Liu, Hongsheng Li, Yu Qiao, Wenlong Zhang, <strong>Xi Chen*</strong>, Yihao Liu*
                  <br>
                  <em>Arxiv Preprint</em>, 2025

                  <br>
                  <a href="https://arxiv.org/abs/2510.17681">pdf</a> /
                  <a href="https://picabench.github.io/">page</a> /
                  <a href="https://github.com/Andrew0613/PICABench">code</a> 
                  <p></p>
                  <p>
                    We evaluates physical realism across eight sub-dimension for most of the common editing operations (add, remove, attribute change, etc.)
                    We find that even SoTA models could not deal with physics well. 
                  </p>
                </td>
              </tr>   
              

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/physmaster.gif' width="190">
                    </div>
                    <img src='images/physmaster.gif' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2510.13809">
                    <papertitle>PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning</papertitle>
                  </a>
                  <br>
                    Sihui Ji, <strong>Xi Chen</strong>, Xin Tao, Pengfei Wan, Hengshuang Zhao
                  <br>
                  <em>Arxiv Preprint</em>, 2025

                  <br>
                  <a href="https://arxiv.org/abs/2510.13809">pdf</a> /
                  <a href="https://sihuiji.github.io/PhysMaster-Page/">page</a> 
                  <p></p>
                  <p>
                  We investigate physically-plausible image-to-video generation. We train a Physical Encoder to extract a “physical representation” from the initial image using reinforcement learning. 
                  This representation is then injected into a diffusion model to guide and enhance the generation process.
                  </p>
                </td>
              </tr>   

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/PlayerOne.gif' width="190">
                    </div>
                    <img src='images/PlayerOne.gif' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2506.09995">
                    <papertitle>PlayerOne: Egocentric World Simulator</papertitle>
                  </a>
                  <br>
                    Yuanpeng Tu, Hao Luo, <strong>Xi Chen</strong>, Xiang Bai, Fan Wang, Hengshuang Zhao
                  <br>
                  <em>NeurIPS(Oral)</em>, 2025

                  <br>
                  <a href="https://arxiv.org/abs/2506.09995">pdf</a> /
                  <a href="https://playerone-hku.github.io/">page</a> 
                  <p></p>
                  <p>
                    We develop an ego-centric world model. User could use their own actions to explore and interact with the virtual world.
                  </p>
                </td>
              </tr>   

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/omnivcus.gif' width="190">
                    </div>
                    <img src='images/omnivcus.gif' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2506.23361">
                    <papertitle>OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions</papertitle>
                  </a>
                  <br>
                  Yuanhao Cai, He Zhang, <strong>Xi Chen*</strong>, Jinbo Xing, Yiwei Hu, Yuqian Zhou, Kai Zhang, Zhifei Zhang, Soo Ye Kim, Tianyu Wang, Yulun Zhang*, Xiaokang Yang, Zhe Lin, Alan Yuille
                  <br>
                  <em>NeurIPS</em>, 2025

                  <br>
                  <a href="https://arxiv.org/abs/2506.23361">pdf</a> /
                  <a href="https://caiyuanhao1998.github.io/project/OmniVCus/">page</a> 
                  <p></p>
                  <p>
                    We conduct a systematic exploration of customized video generation, including a data construction pipeline, a unified model, and a comprehensive benchmark.
                  </p>
                </td>
              </tr>   




            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">

                  <heading>Academic Service</heading>
                  <p>
                    <b>Reviewer / Program Committee Member</b>
                  </p>
                  <ul>
                    <li> CVPR (2021, 2022, 2023, 2024, 2025, 2026)</li>
                    <li> ICCV (2021, 2023, 2025)</li>
                    <li> ECCV (2022,2024)</li>
                    <li> Siggraph / Siggraph Asia (2024, 2025)</li>
                    <li> NeurIPS (2023, 2024, 2025)</li>
                    <li> ICLR (2023, 2025)</li>
                    <li> AAAI (2022, 2023)</li>
                    <li> ACM MM (2024)</li>
                    <li> Journals: IJCV, PR, TCSVT, ect</li>    
                  </ul>

                  <p>
                    <b>Orgnizer / Program Chair</b>
                  </p>
                  <ul>
                    <li><a href="https://higen-2025.github.io/" target="_blank" rel="noopener noreferrer">HiGen (ICCV 2025 Workshop)</a></li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=200&t=tt&d=64wFOrCgVspDjUUVP2vRXujCrmintJYyeSI4nWJgn-c'></script>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's
                    website</a>
                </p>
              </td>
            </tr>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
